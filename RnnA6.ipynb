{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RnnA6.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "tkMrAUYOhv5k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4vJ596G7hzZt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# the number of iterations to train for\n",
        "numTrainingIters = 10000\n",
        "\n",
        "# the number of hidden neurons that hold the state of the RNN\n",
        "hiddenUnits = 1000\n",
        "\n",
        "# the number of classes that we are learning over\n",
        "numClasses = 5\n",
        "\n",
        "# the number of data points in a batch\n",
        "batchSize = 100\n",
        "\n",
        "# the learning rate\n",
        "learningRate = 0.02\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "drW8PeTQh4MO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# this function takes a dictionary (called data) which contains \n",
        "# of (dataPointID, (classNumber, matrix)) entries.  Each matrix\n",
        "# is a sequence of vectors; each vector has a one-hot-encoding of\n",
        "# an ascii character, and the sequence of vectors corresponds to\n",
        "# one line of text.  classNumber indicates which file the line of\n",
        "# text came from.  \n",
        "# \n",
        "# The argument maxSeqLen is the maximum length of a line of text\n",
        "# seen so far.  fileName is the name of a file whose contents\n",
        "# we want to add to data.  classNum is an indicator of the class\n",
        "# we are going to associate with text from that file.  linesToUse\n",
        "# tells us how many lines to sample from the file.\n",
        "#\n",
        "# The return val is the new maxSeqLen, as well as the new data\n",
        "# dictionary with the additional lines of text added\n",
        "def addToData (maxSeqLen, data, fileName, classNum, linesToUse):\n",
        "    #\n",
        "    # open the file and read it in\n",
        "    with open(fileName) as f:\n",
        "        content = f.readlines()\n",
        "    #\n",
        "    # sample linesToUse numbers; these will tell us what lines\n",
        "    # from the text file we will use\n",
        "    # [Note] random_integers genetate a vector with size \"linesToUse\", rand from 0 to len(content)\n",
        "    myInts = np.random.randint (0, len(content) - 1, linesToUse)\n",
        "    #\n",
        "    # i is the key of the next line of text to add to the dictionary\n",
        "    # [Note] dictionary is called \"data\" in this case, so i is the length of dictionary\n",
        "    i = len(data)\n",
        "    #\n",
        "    # loop thru and add the lines of text to the dictionary\n",
        "    for whichLine in myInts.flat: # myInts.flat is a 1-D interator over myInts\n",
        "        #\n",
        "        # get the line and ignore it if it has nothing in it\n",
        "        line = content[whichLine]\n",
        "        if line.isspace () or len(line) == 0:\n",
        "            continue;\n",
        "        #\n",
        "        # take note if this is the longest line we've seen\n",
        "        if len (line) > maxSeqLen:\n",
        "            maxSeqLen = len (line)\n",
        "        #\n",
        "        # create the matrix that will hold this line\n",
        "        temp = np.zeros((len(line), 256))\n",
        "        #\n",
        "        # j is the character we are on\n",
        "        j = 0\n",
        "        # \n",
        "        # loop thru the characters\n",
        "        for ch in line:\n",
        "            #\n",
        "            # non-ascii? ignore\n",
        "            if ord(ch) >= 256: # ord(c) gives the unicode of c\n",
        "                continue\n",
        "            #\n",
        "            # one hot!\n",
        "            temp[j][ord(ch)] = 1 # mark the ascii index \n",
        "            # \n",
        "            # move onto the next character\n",
        "            j = j + 1\n",
        "            #\n",
        "        # remember the line of text\n",
        "        # add this (class number, matrix_of_line) to end of data (dictionary)\n",
        "        data[i] = (classNum, temp)\n",
        "        #\n",
        "        # move onto the next line\n",
        "        i = i + 1\n",
        "    #\n",
        "    # and return the dictionary with the new data\n",
        "    return (maxSeqLen, data) # (max length of the line in file, and the dictionary)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zI5obvCWh8oh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# this function takes as input a data set encoded as a dictionary\n",
        "# (same encoding as the last function) and pre-pends every line of\n",
        "# text with empty characters so that each line of text is exactly\n",
        "# maxSeqLen characters in size\n",
        "def pad (maxSeqLen, data):\n",
        "   #\n",
        "   # loop thru every line of text\n",
        "   for i in data:\n",
        "        #\n",
        "        # access the matrix and the label\n",
        "        temp = data[i][1]\n",
        "        label = data[i][0]\n",
        "        # \n",
        "        # get the number of chatacters in this line\n",
        "        len = temp.shape[0]\n",
        "        #\n",
        "        # and then pad so the line is the correct length\n",
        "        padding = np.zeros ((maxSeqLen - len,256)) \n",
        "        data[i] = (label, np.transpose (np.concatenate ((padding, temp), axis = 0)))\n",
        "   #\n",
        "   # return the new data set\n",
        "   return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k53Xv1Q7h-6T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# this generates a new batch of training data of size batchSize from the\n",
        "# list of lines of text data. This version of generateData is useful for\n",
        "# an RNN because the data set x is a NumPy array with dimensions\n",
        "# [batchSize, 256, maxSeqLen]; it can be unstacked into a series of\n",
        "# matrices containing one-hot character encodings for each data point\n",
        "# using tf.unstack(inputX, axis=2)\n",
        "def generateDataRNN (maxSeqLen, data):\n",
        "    #\n",
        "    # randomly sample batchSize lines of text\n",
        "    myInts = np.random.randint (0, len(data) - 1, batchSize)\n",
        "    #\n",
        "    # stack all of the text into a matrix of one-hot characters\n",
        "    x = np.stack (data[i][1] for i in myInts.flat)\n",
        "    #\n",
        "    # and stack all of the labels into a vector of labels\n",
        "    y = np.stack (np.array((data[i][0])) for i in myInts.flat)\n",
        "    #\n",
        "    # return the pair\n",
        "    return (x, y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ej8_j2p1iBRw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# this also generates a new batch of training data, but it represents\n",
        "# the data as a NumPy array with dimensions [batchSize, 256 * maxSeqLen]\n",
        "# where for each data point, all characters have been appended.  Useful\n",
        "# for feed-forward network training\n",
        "def generateDataFeedForward (maxSeqLen, data):\n",
        "    #\n",
        "    # randomly sample batchSize lines of text\n",
        "    myInts = np.random.randint (0, len(data) - 1, batchSize)\n",
        "    #\n",
        "    # stack all of the text into a matrix of one-hot characters\n",
        "    x = np.stack (data[i][1].flatten () for i in myInts.flat) # flatten turns matrix into 1-D form\n",
        "    #\n",
        "    # and stack all of the labels into a vector of labels\n",
        "    y = np.stack (np.array((data[i][0])) for i in myInts.flat)\n",
        "    #\n",
        "    # return the pair\n",
        "    return (x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oyj54m4WiDJz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create the data dictionary\n",
        "maxSeqLen = 0\n",
        "data = {}\n",
        "\n",
        "# load up the five data sets\n",
        "(maxSeqLen, data) = addToData (maxSeqLen, data, \"biochemistry_processed.txt\", 0, 10000)\n",
        "(maxSeqLen, data) = addToData (maxSeqLen, data, \"cancerResearch_processed.txt\", 1, 10000)\n",
        "(maxSeqLen, data) = addToData (maxSeqLen, data, \"jama_processed.txt\", 2, 10000)\n",
        "(maxSeqLen, data) = addToData (maxSeqLen, data, \"nature_processed.txt\", 3, 10000)\n",
        "(maxSeqLen, data) = addToData (maxSeqLen, data, \"plosOne_processed.txt\", 4, 10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sa1nEXNFjHh5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# pad each entry in the dictionary with empty characters as needed so\n",
        "# that the sequences are all of the same length\n",
        "data = pad (maxSeqLen, data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q1CvM5VSmiuu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(data[1200][1])\n",
        "#data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "je0d9I_GiFdX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# now we build the TensorFlow computation... there are two inputs, \n",
        "# a batch of text lines and a batch of labels\n",
        "inputX = tf.placeholder(tf.float32, [batchSize, 256, maxSeqLen])\n",
        "inputY = tf.placeholder(tf.int32, [batchSize])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B-T2-UObjDZN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# this is the inital state of the RNN, before processing any data\n",
        "initialState = tf.placeholder(tf.float32, [batchSize, hiddenUnits])\n",
        "\n",
        "# the weight matrix that maps the inputs and hidden state to a set of values\n",
        "W = tf.Variable(np.random.normal(0, 0.05, (hiddenUnits + 256, hiddenUnits)), dtype=tf.float32)\n",
        "\n",
        "# biaes for the hidden values\n",
        "b = tf.Variable(np.zeros((1, hiddenUnits)), dtype=tf.float32)\n",
        "\n",
        "# weights and bias for the final classification\n",
        "W2 = tf.Variable(np.random.normal (0, 0.05, (hiddenUnits, numClasses)),dtype=tf.float32)\n",
        "b2 = tf.Variable(np.zeros((1,numClasses)), dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S7oqo94HnHtq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7GJK2g_7n2OW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# unpack the input sequences so that we have a series of matrices,\n",
        "# each of which has a one-hot encoding of the current character from\n",
        "# every input sequence\n",
        "sequenceOfLetters = tf.unstack(inputX, axis=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IBF3nUElsaQx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sequenceOfLetters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RsgoX92GsbQY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# now we implement the forward pass\n",
        "currentState = initialState\n",
        "for timeTick in sequenceOfLetters:\n",
        "    #\n",
        "    # concatenate the state with the input, then compute the next state\n",
        "    inputPlusState = tf.concat([timeTick, currentState], 1)  \n",
        "    next_state = tf.tanh(tf.matmul(inputPlusState, W) + b) \n",
        "    currentState = next_state\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "drOCiGMIspA4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "initialState"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KNtDEkfIsxtm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for timeRick in sequenceOfLetters:\n",
        "  print(timeRick)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "klyyDa77s4c2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# compute the set of outputs\n",
        "outputs = tf.matmul(currentState, W2) + b2 # matmul\n",
        "\n",
        "predictions = tf.nn.softmax(outputs) # softmax\n",
        "\n",
        "# compute the loss\n",
        "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=outputs, labels=inputY)\n",
        "totalLoss = tf.reduce_mean(losses)\n",
        "\n",
        "# use gradient descent to train\n",
        "#trainingAlg = tf.train.GradientDescentOptimizer(learningRate).minimize(totalLoss)\n",
        "trainingAlg = tf.train.AdagradOptimizer(learning_rate=learningRate).minimize(totalLoss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cEUEcMt5yx92",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# and train!!\n",
        "with tf.Session() as sess:\n",
        "    #\n",
        "    # initialize everything\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    #\n",
        "    # and run the training iters\n",
        "    _accLoss = 0.0\n",
        "    _accCount = 0\n",
        "    for epoch in range(numTrainingIters):\n",
        "        # \n",
        "        # get some data\n",
        "        x, y = generateDataRNN (maxSeqLen, data)\n",
        "        #\n",
        "        # do the training epoch\n",
        "        _currentState = np.zeros((batchSize, hiddenUnits))\n",
        "        _totalLoss, _trainingAlg, _currentState, _predictions, _outputs = sess.run(\n",
        "                [totalLoss, trainingAlg, currentState, predictions, outputs],\n",
        "                feed_dict={\n",
        "                    inputX:x,\n",
        "                    inputY:y,\n",
        "                    initialState:_currentState\n",
        "                })\n",
        "        #\n",
        "        # just FYI, compute the number of correct predictions\n",
        "        numCorrect = 0\n",
        "        for i in range (len(y)):\n",
        "            maxPos = -1\n",
        "            maxVal = 0.0\n",
        "            for j in range (numClasses):\n",
        "                if maxVal < _predictions[i][j]:\n",
        "                    maxVal = _predictions[i][j]\n",
        "                    maxPos = j\n",
        "            if maxPos == y[i]:\n",
        "                numCorrect = numCorrect + 1\n",
        "        #\n",
        "        # print out to the screen\n",
        "        if epoch%100 == 0: \n",
        "            print(\"Step\", epoch, \"Loss\", _totalLoss, \"Correct\", numCorrect, \"out of\", batchSize)\n",
        "\n",
        "        if epoch >= numTrainingIters - 10:\n",
        "            _accLoss += _totalLoss\n",
        "            _accCount += numCorrect \n",
        "    \n",
        "    print(\"Average loss for the last 10 mini-batches is\", _accLoss / 10, \n",
        "          \"average correct labels is\", _accCount / 10, \"out of 100.\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}